name: Scrape Instagram and Store Events

on:
  schedule:
    - cron: '0 7 * * *'   # Once per day at 07:00 UTC
  workflow_dispatch:      # Optional manual trigger

jobs:
  instagram_feed:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend
    env:
      SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
      USERNAME: ${{ secrets.USERNAME }}
      PASSWORD: ${{ secrets.PASSWORD }}
      SESSIONID: ${{ secrets.SESSIONID }}
      CSRFTOKEN: ${{ secrets.CSRFTOKEN }}
      DS_USER_ID: ${{ secrets.DS_USER_ID }}
      IG_DID: ${{ secrets.IG_DID }}
      MID: ${{ secrets.MID }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      DOC_ID: ${{ secrets.DOC_ID }}
      USER_AGENT: ${{ secrets.USER_AGENT }}
      X_IG_APP_ID: ${{ secrets.X_IG_APP_ID }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('backend/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install build tools
        run: pip install --upgrade pip setuptools wheel

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Create logs directory
        run: mkdir -p logs

      - name: Run scraper
        run: |
          cd backend/scraping
          python -u instagram_feed.py 2>&1 | tee ../../logs/scraping.log
        continue-on-error: true

      - name: Upload logs as artifacts
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: scraping-logs-${{ github.run_number }}
          path: |
            logs/
            backend/scraping/*.log
            backend/scraping/*.csv
          retention-days: 30